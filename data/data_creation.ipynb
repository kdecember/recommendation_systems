{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading, reading, and exporting the contents to CSV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the data has been downloaded from https://cseweb.ucsd.edu/~jmcauley/datasets.html#steam_data,\n",
    "unzip the files and move the resulting JSONs to this folder, do not rename them\n",
    "\n",
    "Running the first cell is all that's required for running the [final notebook](../notebooks/report/final_notebook.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "games = []\n",
    "\n",
    "with open('steam_games.json', 'r') as f:\n",
    "    for l in f:\n",
    "        games.append(eval(l))\n",
    "        \n",
    "        \n",
    "df_games = pd.DataFrame(games)\n",
    "df_games.to_csv('games.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning, Subsampling, Aggregating, and Exporting Review Text\n",
    "\n",
    "    please note, this is not required as the subsample_agg_reviews.p is already located in the repo, these cells will also take some time to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = []\n",
    "\n",
    "with open('steam_reviews.json', 'r') as f:\n",
    "    for l in f:\n",
    "        reviews.append(eval(l))\n",
    "        \n",
    "        \n",
    "df_reviews = pd.DataFrame(reviews)\n",
    "df_reviews.to_csv('reviews.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop unnecessary columns\n",
    "df_reviews.drop(['found_funny', 'compensation', 'user_id', 'Unnamed: 0', 'products', 'page_order',\\\n",
    "                'date', 'early_access', 'page'], axis=1, inplace=True)\n",
    "\n",
    "# create a frequency column based on product_id, sort by said column\n",
    "df_reviews['freq'] = df_reviews.groupby('product_id')['product_id'].transform('count')\n",
    "df_reviews.sort_values(by=['freq', 'product_id'], ascending=[False, True], inplace=True)\n",
    "\n",
    "# remove null values\n",
    "df_reviews.dropna(inplace=True)\n",
    "\n",
    "# remove reviews by users that had under 1 hour played for the game\n",
    "df_reviews = df_reviews[df_reviews['hours'] >= 1]\n",
    "\n",
    "# remove games that have less than 500 total reviews\n",
    "df_reviews = df_reviews[df_reviews['freq'] >= 500]\n",
    "\n",
    "# convert product_id to strings because Doc2Vec needs strings as Tags\n",
    "df_reviews['product_id'] = df_reviews['product_id'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take subsample of data for text manipulation/modeling purposes\n",
    "df_sample = df_reviews.sample(axis=0, n=250000)\n",
    "df_sample.sort_values(by=['freq', 'product_id'], ascending=[False, True], inplace=True)\n",
    "\n",
    "# make lowercase\n",
    "df_sample['text'] = df_sample['text'].str.lower()\n",
    "\n",
    "# remove new line indicators\n",
    "df_sample['text'] = df_sample['text'].str.replace('\\n', ' ')\n",
    "df_sample['text'] = df_sample['text'].str.replace('.\\n', ' ')\n",
    "\n",
    "# tokenize text\n",
    "df_sample['tokens'] = df_sample['text'].apply(nltk.word_tokenize)\n",
    "df_sample['tokens']\n",
    "\n",
    "# join tokens into single string\n",
    "df_sample['clean_text'] = df_sample['tokens'].apply(', '.join)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create product list of the unique product ids\n",
    "products = list(df_sample['product_id'].unique())\n",
    "\n",
    "# initiate a dictionary where each key is a unique product id, and the value is the aggregated text\n",
    "product_dict = {}\n",
    "for product_id in products:\n",
    "    product_dict[product_id] = ''\n",
    "\n",
    "# aggregate the text from each review in the dataframe corresponding to the product id key\n",
    "for key in product_dict:\n",
    "    for index, row in df_sample[df_sample['product_id'] == key].iterrows():\n",
    "        product_dict[key] = product_dict[key] + ' ' + row['clean_text']\n",
    "\n",
    "# export the aggregated text to a pickled file\n",
    "with open('subsample_agg_reviews.p', 'wb') as fp:\n",
    "    pickle.dump(product_dict, fp, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-env",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
